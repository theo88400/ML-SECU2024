{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML for cybersecurity - Exploration\n",
    "\n",
    "## Project objectives\n",
    "\n",
    "The goal of the project is to design, deploy and evaluate a data chain for the analysis of\n",
    "cybersecurity data. The data treatment will be performed as batch.\n",
    "\n",
    "This notebook will be used to explore the data and try to find some interesting insights.\n",
    "\n",
    "### Data description\n",
    "\n",
    "This paper presents a dataset to support researchers in the validation process of solutions such\n",
    "as Intrusion Detection Systems (IDS) based on artificial intelligence and machine learning techniques for\n",
    "the detection and categorization of threats in Cyber Physical Systems (CPS). To this end, data were acquired\n",
    "from a hardware-in-the-loop Water Distribution Testbed (WDT) which emulates water flowing between eight\n",
    "tanks via solenoid-valves, pumps, pressure and flow sensors. The testbed is composed of a real subsystem\n",
    "that is virtually connected to a simulated one. The proposed dataset encompasses both physical and network\n",
    "data in order to highlight the consequences of attacks in the physical process as well as in network traffic\n",
    "behaviour. Simulations data are organized in four different acquisitions for a total duration of 2 hours by\n",
    "considering normal scenario and multiple anomalies due to cyber and physical attacks.\n",
    "\n",
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_data import get_HITL, clean_HITL\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random_state = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "As both types of datasets have different structures, we will load them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitl_dict = get_HITL(\"../../data/HardwareInTheLoop/\", small=True)\n",
    "df_network, df_physical = clean_HITL(hitl_dict) # Clean-up helper function\n",
    "\n",
    "print(\"Network dataset shape: \", df_network.shape)\n",
    "print(\"Physical dataset shape: \", df_physical.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_network.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_network.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `attack` column come from the cleanup of the dataset to differentiate between normal and attack datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dtypes of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_network.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the proportion of nan values per column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_network.isna().sum() / df_network.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`modbus_response` is made out of more than half of nan values, trying to remove this column later on might be a good idea to reduce the dimensionality of the dataset.\n",
    "\n",
    "`sport`, `dport` and `flags` are all numerical columns so we can interpolate some values to fill the nan values later on. Theses columns seem to be interesting to play with/without to see how they impact the model.\n",
    "\n",
    "A PCA could be interesting to reduce the dimensionality of the dataset as 5 columns contain lots of nan values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels\n",
    "\n",
    "Let's take a quick detour to see what are the labels like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_dataset_labels = df_network[[\"label_n\", \"label\", \"attack\"]]\n",
    "df_network = df_network.drop(columns=[\"label_n\", \"label\", \"attack\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_dataset_labels.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_dataset_labels[\"label_n\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_dataset_labels.loc[:, [\"label_n\"]].plot(kind='density', subplots=True, layout=(1,1), sharex=False, figsize=(7, 7), title=\"Density Plot of Network Dataset Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is unbalanced, the ratio seems to be 1/3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_dataset_labels[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 5 labels: normal, DoS, MITM, physical fault, anomaly.\n",
    "More imporantly the data is also split is normal (label_n=0) or not normal (label_n=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset labels value counts\n",
    "network_dataset_labels_value_counts = network_dataset_labels[\"label\"].value_counts()\n",
    "\n",
    "# Ratio of each label\n",
    "network_dataset_labels_value_counts = network_dataset_labels_value_counts / network_dataset_labels_value_counts.sum() * 100\n",
    "\n",
    "# Bar plot of the dataset labels\n",
    "network_dataset_labels_value_counts.plot(kind=\"bar\", title=\"Percentage of Labels (Network Dataset)\", figsize=(7, 7), rot=45, legend=True, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Numbers columns:__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_number_cols = df_network.select_dtypes(include=['number']).columns.values.tolist()\n",
    "network_number_cols.remove(\"time\") # Timestamp column\n",
    "network_number_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_network[network_number_cols].plot(kind='density', subplots=True, layout=(3,3), sharex=False, figsize=(10, 10), title=\"Density Plot of Network Dataset Numerical Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly analyse the density of the numerical columns:\n",
    "- `sport` and `dport`: two density spikes around 0 and 60000\n",
    "- `flags`: a huge spike around 11000 and very few data around 10000 and 0\n",
    "- `size`: two main spikes, the first one around 60 and the second one is around 1500, but 10x less dense\n",
    "- `n_pkt_src` and `n_pkt_dst`: closer data with more dominant spikes, the two main ones are around 15 and 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_network.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly analyse this:\n",
    "- `n_pkt` columns range from 0 to a 100. Combined to what we saw on the density plot, we might be able to cluster them and reduce the dimensionality of the dataset.\n",
    "- `flags` flags has a median of 10902 which is extremely close of the max value (which represents most of the density).\n",
    "- `ports` look the same (density wise and range wise) but on a different scale. We might be able to cluster them as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Object columns:__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_categorical_cols = df_network.select_dtypes(include=['object']).columns.values.tolist()\n",
    "network_categorical_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the distribution of the object columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [col for col in network_categorical_cols if col not in [\"modbus_response\", \"Time\"]] # Remove modbus_response (too many NaNs) and Time columns (not fit for this analysis)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "fig.suptitle(\"Bar Plot of Network Dataset Categorical Features\", fontsize=20)\n",
    "for i, col in enumerate(cols):\n",
    "    ax = fig.add_subplot(3, 3, i+1)\n",
    "    value_count = df_network[col].value_counts()\n",
    "    lines = value_count.plot(kind='bar', ax=ax)\n",
    "    for j, patch in enumerate(ax.patches):\n",
    "        patch.set_facecolor(f\"C{j}\")\n",
    "        patch.set_label(value_count.index[j])\n",
    "    ax.legend(handles=ax.patches)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to deduce anything from the distributions, but we make the following observations:\n",
    "- the distribution of `mac_s`, `mac_d`, `ip_s` and `ip_d` are very similar: one address is more frequent than the 7 others\n",
    "- the distribution of `proto` is very unbalanced, with the \"Modbus\" protocol being 10x more frequent than the others. This column might be irrelevant for the classification\n",
    "- the `modbus_response` data is very balanced between the 4 values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most of our categorical features have <10 unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hist_pairs_per_column(dataset, col_name_1, col_name_2):\n",
    "    # Sum each identical pair of ip addresses\n",
    "    ip_pairs_count = {}\n",
    "    for i, row in dataset.iterrows():\n",
    "        ip_pair = (row[col_name_1], row[col_name_2])\n",
    "        if ip_pair in ip_pairs_count:\n",
    "            ip_pairs_count[ip_pair] += 1\n",
    "        else:\n",
    "            ip_pairs_count[ip_pair] = 1\n",
    "\n",
    "    # Numerize each pair to print it on a graph\n",
    "    ip_pairs_count_numerized = {}\n",
    "    for i, (key, value) in enumerate(ip_pairs_count.items()):\n",
    "        ip_pairs_count_numerized[i] = value\n",
    "\n",
    "    # Plot the histogram\n",
    "    plt.bar(ip_pairs_count_numerized.keys(), ip_pairs_count_numerized.values())\n",
    "    plt.title(f\"Number of occurences of each pair of {col_name_1} and {col_name_2}\")\n",
    "    plt.xlabel(\"Pair\")\n",
    "    plt.ylabel(\"Number of occurences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hist_pairs_per_column(df_network, \"mac_s\", \"mac_d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have around 30 pairs of (mac_s, mac_d) that are unique. There are mostly 7-9 pairs that are mostly used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hist_pairs_per_column(df_network, \"ip_s\", \"ip_d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have around 21 pairs of (ip_s, ip_d) that are unique. The repartition here is also a bit shallow as there are 7-9 pairs that are also mostly used. The repartition of the ip addresses is very similar to the mac addresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between features\n",
    "\n",
    "We'll first take a look at the correlation between the features that are the same types (numerical or categorical).\n",
    "\n",
    "And then'll we'll broaden our analysis to all the features together to try and catch some interesting insights like maybe the correlation between the packets size and the port used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlsecu.data_exploration_utils import (\n",
    "    get_number_column_names,\n",
    "    get_object_column_names,\n",
    ")\n",
    "from mlsecu.data_preparation_utils import (\n",
    "    get_one_hot_encoded_dataframe,\n",
    "    remove_nan_through_mean_imputation,\n",
    ")\n",
    "\n",
    "# We'll remove the modbus_response column as it is has too many NaN values\n",
    "# it would be hard to impute as we have so much categorical data\n",
    "df_network = df_network.drop(columns=[\"modbus_response\"])\n",
    "\n",
    "# Get the number columns\n",
    "number_cols = get_number_column_names(df_network)\n",
    "number_cols.remove(\"time\") # Timestamp column\n",
    "# Get the object columns\n",
    "object_cols = get_object_column_names(df_network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute and display the correlation matrix of the numbers columns first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_network[number_cols] = remove_nan_through_mean_imputation(df_network[number_cols])\n",
    "\n",
    "corr_matrix = df_network[number_cols].corr(method='spearman').abs()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation matrix with sns\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"Correlation matrix of numerical features\")\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The analysis reveals the following relationships between the different variables:\n",
    "\n",
    "1. **`sport` and `dport`**:\n",
    "   - These two variables continue to show a strong positive correlation (\\(0.913909\\)), indicating that the values of `sport` and `dport` tend to increase or decrease together.\n",
    "\n",
    "2. **`sport`, `dport`, and `size`**:\n",
    "   - `sport` and `size` exhibit a moderate positive correlation (\\(0.667260\\)).\n",
    "   - `dport` and `size` also show a moderate positive correlation (\\(0.667895\\)).\n",
    "   - These correlations suggest that increases in the values of `sport` and `dport` are generally associated with increases in the packet size (`size`).\n",
    "\n",
    "3. **`size` and `n_pkt_src`**:\n",
    "   - These two variables have a relatively strong positive correlation (\\(0.752641\\)), indicating that increases in packet size are often associated with increases in the number of packets originating from the source.\n",
    "\n",
    "4. **`flags`**:\n",
    "   - `flags` shows moderate positive correlations with `n_pkt_src` (\\(0.493689\\)) and `n_pkt_dst` (\\(0.495567\\)), and a weaker correlation with `size` (\\(0.385702\\)).\n",
    "   - This suggests that increases in the values of `flags` are generally associated with increases in the number of packets originating from the source, the number of packets destined, and the packet size.\n",
    "\n",
    "5. **`n_pkt_src` and `n_pkt_dst`**:\n",
    "   - These two variables show a very weak positive correlation (\\(0.062721\\)), indicating that the number of packets originating from the source and the number of packets destined are not strongly linked.\n",
    "\n",
    "6. **`sport`, `dport`, `n_pkt_src`, and `n_pkt_dst`**:\n",
    "   - These variables show moderate positive correlations among themselves, suggesting some relationship in their movements.\n",
    "\n",
    "In summary, the strongest relationships are observed between `sport` and `dport`, as well as between `size` and `n_pkt_src`. The other correlations are more moderate or weak, indicating less direct relationships between these variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the correlation between the categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply label encoding to all object columns\n",
    "# If we use one-hot encoding, we will have a correlation matrix with almost 1000+ columns and lines\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cpy_df_network = df_network.copy()\n",
    "label_encoder = LabelEncoder()\n",
    "for col in object_cols:\n",
    "    cpy_df_network[col] = label_encoder.fit_transform(df_network[col])\n",
    "cpy_df_network[object_cols].head()\n",
    "\n",
    "cpy_df_network = remove_nan_through_mean_imputation(cpy_df_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr_matrix = cpy_df_network[object_cols].corr(method='spearman').abs()\n",
    "corr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"Correlation matrix of categorical features\")\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis of the provided correlation matrix reveals the following insights:\n",
    "\n",
    "1. **`ip_s` and `ip_d`**:\n",
    "   - These variables show a strong positive correlation (\\(0.834745\\)), suggesting that the source and destination IP addresses are often related.\n",
    "\n",
    "2. **`proto`**:\n",
    "   - `proto` exhibits moderate positive correlations with `mac_s` (\\(0.578621\\)) and `mac_d` (\\(0.516733\\)), and a slightly lower correlation with `modbus_fn` (\\(0.463579\\)).\n",
    "   - This indicates that changes in the `proto` value are somewhat associated with changes in the source and destination MAC addresses, as well as the Modbus function code.\n",
    "\n",
    "3. **`modbus_fn`**:\n",
    "   - `modbus_fn` shows moderate positive correlations with `ip_s` (\\(0.369480\\)) and `mac_d` (\\(0.353727\\)), suggesting that the Modbus function code tends to change with variations in these values.\n",
    "\n",
    "4. **`mac_s` and `mac_d`**:\n",
    "   - These variables have a moderate positive correlation (\\(0.331400\\)), indicating that there is some relationship between the source and destination MAC addresses.\n",
    "\n",
    "5. **Other Correlations**:\n",
    "   - The other pairs of variables show weak correlations, suggesting that there are no strong linear relationships between them.\n",
    "\n",
    "Overall, the most significant relationship is observed between `ip_s` and `ip_d`, while other variables have moderate or weak correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's look at the correlation between all the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = cpy_df_network.corr(method='spearman').abs() # All the features columns are already enocded\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "fig.suptitle(\"Correlation matrix of all features\", fontsize=16)\n",
    "fig.tight_layout()\n",
    "im = ax.matshow(corr_matrix)\n",
    "fig.colorbar(im)\n",
    "ticks = np.arange(0, len(corr_matrix.columns))\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(cpy_df_network.columns.tolist(), rotation=80)\n",
    "ax.set_yticklabels(cpy_df_network.columns.tolist())\n",
    "\n",
    "for (i, j), z in np.ndenumerate(corr_matrix):\n",
    "    ax.text(j, i, '{:0.1f}'.format(z), ha='center', va='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole correlation matrix is very hard to read, so we'll focus on the most interesting parts.\n",
    "We can see the same anaylysis as before, from the two separated kinds of features but we can also see that the algorithm is struggling to find any correlation between the two types of features. This is surely due to how we encoded our categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to go a bit deeper and plot the labels as a time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_network[network_dataset_labels.columns] = network_dataset_labels\n",
    "\n",
    "df_network_sorted = df_network.sort_values(by=[\"time\"])\n",
    "df_network_sorted[\"time_minute\"] = pd.to_datetime(df_network_sorted[\"time\"], unit='s').dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "df_network_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df_network_sorted.groupby([\"time_minute\", 'label_n']).size().unstack().fillna(0)\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped.plot(kind='bar', figsize=(20, 10), xticks=range(0, df_grouped.shape[0], 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's see the number of anormal data per minute for the \"normal\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped.loc[\"2021-04-09 11\":\"2021-04-09 13\"].plot(figsize=(15, 7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there is no 1 label in the normal dataset. Let's check for 2020-04-09 (first day of recorded anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped.loc[\"2021-04-09 18\":\"2021-04-09 22\"].plot(figsize=(15, 7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see way more fluctuation in the data. There is anormal activity for some time, then it stops, then it starts again with a huge peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped.loc[\"2021-04-19\":\"2021-04-20\"].plot(figsize=(15, 7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physical dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_physical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_physical.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dtypes of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_physical.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the proportion of nan values per column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_physical.isna().sum() / df_physical.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprinsigly, coming from a dataset that has been made from physical data capture, this dataset has no NaN values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels\n",
    "\n",
    "Let's take a quick detour to see what are the labels like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_dataset_labels = df_physical[[\"label_n\", \"label\"]]\n",
    "df_physical = df_physical.drop(columns=[\"label_n\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_dataset_labels.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_dataset_labels[\"label_n\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_dataset_labels.plot(kind='density', subplots=True, layout=(1,1), sharex=False, figsize=(7, 7), title=\"Density Plot of Physical Dataset Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is unbalanced, the ratio seems to be 1/4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_dataset_labels[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset labels value counts\n",
    "physical_dataset_labels_value_counts = physical_dataset_labels[\"label\"].value_counts()\n",
    "\n",
    "# Ratio of each label\n",
    "physical_dataset_labels_value_counts = physical_dataset_labels_value_counts / physical_dataset_labels_value_counts.sum() * 100\n",
    "\n",
    "# Bar plot of the dataset labels\n",
    "physical_dataset_labels_value_counts.plot(kind=\"bar\", title=\"Percentage of Labels (Physical Dataset)\", figsize=(7, 7), rot=45, legend=True, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 5 labels: normal, DoS, MITM, physical fault, anomaly.\n",
    "More imporantly the data is also split is normal (label_n=0) or not normal (label_n=1).\n",
    "\n",
    "This is the same as our network dataset.\n",
    "\n",
    "However, the most noticeable thing is that the physical dataset has less than a hundred samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all([t in [int, float] for t in df_physical.dtypes])\n",
    "len(df_physical.columns) # 42\n",
    "df_physical.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our features are numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_physical.plot(subplots=True, layout=(6,7), sharex=False, figsize=(20, 20), title=\"Plot of Physical Dataset Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some features are constrant troughout the dataset, let's remove them and look at the density of the rest of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_removed_cols = 0\n",
    "for col in df_physical.columns:\n",
    "    unique_values = df_physical.loc[:, col].value_counts()\n",
    "    if len(unique_values) == 1:\n",
    "        df_physical.drop(columns=col, inplace=True)\n",
    "        n_removed_cols += 1\n",
    "print(\"Number of columns removed: \", n_removed_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the density to have a better look at the data repartition\n",
    "df_physical.plot(kind=\"density\", subplots=True, layout=(6,5), sharex=False, figsize=(20, 20), title=\"Density Plot of Physical Dataset Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly analyse the density of the numerical columns:\n",
    "- `tank`: most of the data is around 0. The sensors 2, 5 and 6 have non-zero values that are proportionnaly bigger or on par with the 0 values.\n",
    "- `pump`: boolean, 0 or 1. 0 is 2x/3x more dense than 1.\n",
    "- `flow_sensor`: int, between 0 and 6000. Most of the data is 0, all sensors have a spike around 4000.\n",
    "- `valv`: boolean, 0 or 1. The distribution between the two values differ a lot between the sensors.\n",
    "\n",
    "\n",
    "Now, let's look at the correlation between the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr_matrix = df_physical.corr(method='spearman').abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation matrix with sns\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.title(\"Correlation matrix of Physical Dataset Features\")\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the matrix is way to big to be read easily.\n",
    "There doesn´t seem to be any correlation between the four sensors.\n",
    "\n",
    "As expected, we can see that some groups of the same sensors are correlated together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side note: Time series\n",
    "\n",
    "The data is recorded at two points in time. Let's cluster them to see what we can find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the time column\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "time_col = df_physical[\"time\"].values.reshape(-1, 1)\n",
    "scaler = StandardScaler()\n",
    "time_col_scaled = scaler.fit_transform(time_col)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=random_state)\n",
    "\n",
    "kmeans.fit(time_col_scaled)\n",
    "\n",
    "df_physical[\"time_cluster\"] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that we have two clusters. Is is the same for the network dataset. We can see that in the density plot of the number values of the network dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to our physical dataset, let's look at the time series of the sensors.\n",
    "\n",
    "Let's try to scatter plot the data of each sensor category combined relative to the timestep of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scatter_chart(df, sensor_list, logx=False, logy=False):\n",
    "    subplot_size = int(np.sqrt(len(sensor_list)))\n",
    "    fig, axs = plt.subplots(subplot_size, subplot_size, figsize=(30, 30))\n",
    "    fig.suptitle(\"Scatter chart of each sensor type\", fontsize=20)\n",
    "\n",
    "    # For each sensor type, plot the scatter chart\n",
    "    for sensor_prefix in sensor_list:\n",
    "        columns = df.columns[df.columns.str.startswith(sensor_prefix)]\n",
    "\n",
    "        # Create a scatter chart with each column in columns with a different color\n",
    "        ax = axs.flatten()[sensor_list.index(sensor_prefix)]\n",
    "        is_bool = True\n",
    "        for i, col in enumerate(columns):\n",
    "            if len(np.unique(df[col])) > 2:\n",
    "                is_bool = False\n",
    "                break\n",
    "\n",
    "        for i, col in enumerate(columns):\n",
    "            df.plot(kind=\"scatter\", x=\"time_readable\", y=col, color=f\"C{i}\", ax=ax, label=col, logx=logx, logy=logy if not is_bool else False)\n",
    "        \n",
    "\n",
    "        # Plot it\n",
    "        ax.tick_params(axis='x', labelrotation = 45) # Better readability\n",
    "        ax.set_title(sensor_prefix[:-1])\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"Value\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cluster = df_physical[df_physical[\"time_cluster\"] == 0]\n",
    "\n",
    "# Compute the max span of time col in readable format\n",
    "max_span = first_cluster[\"time\"].max() - first_cluster[\"time\"].min()\n",
    "max_span_readable = pd.to_datetime(max_span, unit='s').strftime(\"%H:%M:%S\")\n",
    "print(\"Max span of time col in First cluster: \", max_span_readable)\n",
    "\n",
    "# Get the time in MM:SS format\n",
    "x_axis_time = pd.to_datetime(first_cluster.loc[:, \"time\"], unit='s').dt.strftime(\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a scatter chart for every type of sensor in the first cluster\n",
    "sensor_list = [\"tank_\", \"pump_\", \"flow_sensor_\", \"valv_\"]\n",
    "first_cluster.loc[:, \"time_readable\"] = x_axis_time\n",
    "print_scatter_chart(first_cluster, sensor_list, logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There seem to be a pattern in the values for `tank_2` ,`tank_3`, `tank_5` and `tank_7`. Very few values are low.\n",
    "- `pump` has 7 occurences where none of them are True.\n",
    "- `flow_sens_4` seems to be the one that varies the most.\n",
    "- `valv`s have only one occurence where none of them are False. This is surely due to the amount of sensors we had in this category\n",
    "\n",
    "Second Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_cluster = df_physical[df_physical[\"time_cluster\"] == 0]\n",
    "\n",
    "# Compute the max span of time col in readable format\n",
    "max_span = second_cluster[\"time\"].max() - second_cluster[\"time\"].min()\n",
    "max_span_readable = pd.to_datetime(max_span, unit='s').strftime(\"%H:%M:%S\")\n",
    "print(\"Max span of time col in First cluster: \", max_span_readable)\n",
    "\n",
    "# Get the time in MM:SS format\n",
    "x_axis_time = pd.to_datetime(second_cluster.loc[:, \"time\"], unit='s').dt.strftime(\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a scatter chart for every type of sensor\n",
    "sensor_list = [\"tank_\", \"pump_\", \"flow_sensor_\", \"valv_\"]\n",
    "second_cluster.loc[:, \"time_readable\"] = x_axis_time\n",
    "print_scatter_chart(second_cluster, sensor_list, logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The behavior of the second cluster is very similar to the first one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset, originating from the Water Distribution Testbed, encompasses both physical and network data, essential for understanding the impact of attacks on the physical process and network traffic. In the network dataset analysis, we examined feature distributions, laying the foundation for feature engineering and model building.\n",
    "\n",
    "Subsequently, in the physical dataset, we were pleasantly surprised to find no missing values, albeit with label imbalance issues. Our features underwent density analysis, highlighting patterns in the different system points of capture: tanks, pumps, flow sensors, and valves.\n",
    "\n",
    "In addition, we delved into the dataset's time series aspects, clustering the data for in-depth examination.\n",
    "\n",
    "Overall, these analyses provide crucial insights for our ongoing project, aiding in feature selection, preprocessing, and subsequent modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
